{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[H\u001b[2JPresented by \n","               __  __ _   _ ____ ___    ____                       \n","             |  \\/  | | | |  _ \\_ _|  / ___|_ __ ___  _   _ _ __  \n","             | |\\/| | |_| | |_) | |  | |  _| '__/ _ \\| | | | '_ \\ \n","             | |  | |  _  |  __/| |  | |_| | | | (_) | |_| | |_) |\n","             |_|  |_|_| |_|_|  |___|  \\____|_|  \\___/ \\__,_| .__/ \n","                                                           |_|    \n","\n","Device: cpu\n","\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x1503bc6c16d0>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import display, clear_output\n","import numpy as np\n","import torch\n","import sys\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, random_split, dataset\n","import sourcedefender\n","import random\n","   \n","sys.path.append(\"../../../\")\n","sys.path.append(\"../../\")\n","sys.path.append(\"../\")\n","sys.path.append(\"./\")\n","\n","from lib.util import MHPI, count_parameters\n","from lib.utiltools import loss_live_plot, GaussianRandomFieldGenerator, generate_batch_parameters, AutomaticWeightedLoss\n","from lib.DerivativeComputer import batchJacobian_AD\n","from models.FNO_1d_simple import FNO1d\n","from equation.ode1 import *\n","\n","MHPI()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Device: {device}\\n')\n","\n","# Set seeds\n","random.seed(42)\n","torch.manual_seed(42)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["Sample_number = 1000\n","training_sample = 10\n","dataset_segment_size = 1000"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["plot_live_loss = True\n","Create_new_dataset = True"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["modes = 8\n","width = 20"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["alpha_ = 1.5\n","tau = 0.5"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["t0 = 0.0\n","t_end = 2\n","steps = 100\n","T_in = 10\n","T_out = steps - T_in"]},{"cell_type":"markdown","metadata":{},"source":["optimizer and training configurations"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["epochs = 500\n","batch_size = 16\n","learning_rate = 0.01\n","scheduler_step = 100\n","scheduler_gamma = 0.9"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["t_tensor = torch.linspace(t0, t_end, steps)  # Shape: [Steps]\n","\n","# Define ranges for alpha, beta, and gamma\n","alpha_range = (1, 3)  # Range for alpha\n","beta_range = (1, 3)   # Range for beta\n","gamma_range = (0, 1)  # Range for gamma\n","# Generate random samples for alpha and beta within their respective ranges\n","alpha = alpha_range[0] + (alpha_range[1] - alpha_range[0]) * torch.rand(Sample_number)\n","beta = beta_range[0] + (beta_range[1] - beta_range[0]) * torch.rand(Sample_number)\n","# Generate random samples for gamma within its range\n","gamma = gamma_range[0] + (gamma_range[1] - gamma_range[0]) * torch.rand(Sample_number)\n","\n","parameters = torch.stack([alpha, beta, gamma], dim=1).requires_grad_(True)\n"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset with 1000 samples is created ...!\n"]}],"source":["PATH = ''\n","# Create dataset < This part can be changed based on different cases>\n","if Create_new_dataset:\n","    dataset = creat_dataset(t_tensor, parameters, T_in)\n","    torch.save(dataset, PATH + 'datasets/main_dataset_ODE2.pt')\n","    # Calculate sizes for train/eval/test split\n","    train_size = int(0.75 * Sample_number)\n","    eval_size = int(0.15 * Sample_number)\n","    test_size = Sample_number - train_size - eval_size\n","\n","    # Create DataLoaders\n","    train_dataset, eval_dataset, test_dataset = random_split(dataset, [train_size, eval_size, test_size]) # Split the dataset\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n","    test_loader = DataLoader(test_dataset)\n","    torch.save(train_loader, PATH + 'datasets/train_dataset_ODE2.pt')\n","    torch.save(eval_loader, PATH + 'datasets/eval_dataset_ODE2.pt')\n","    torch.save(test_loader, PATH + 'datasets//test_dataset_ODE2.pt')\n","\n","else:\n","    dataset = torch.load(PATH + 'datasets/main_dataset_ODE2.pt')\n","    train_loader = torch.load(PATH + 'datasets/train_dataset_ODE2.pt')\n","    eval_loader = torch.load(PATH + 'datasets/eval_dataset_ODE2.pt')\n","    test_loader = torch.load(PATH + 'datasets/test_dataset_ODE2.pt')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["FNO1d(\n","  (fc0): Linear(in_features=11, out_features=20, bias=True)\n","  (conv0): SpectralConv1d()\n","  (conv1): SpectralConv1d()\n","  (conv2): SpectralConv1d()\n","  (conv3): SpectralConv1d()\n","  (w0): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n","  (w1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n","  (w2): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n","  (w3): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n","  (fc1): Linear(in_features=23, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",")\n"]}],"source":["model = FNO1d(modes, width, T_in, T_out, state_size=1, parameters_size=3).to(device)\n","print(model)\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["17921"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(model)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["learning_rate = 0.001\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=250, gamma=1.025)\n","train_fnolosses, train_odelosses, train_iglosses, val_losses = [], [], [], []\n","coieffs_list = []\n","criterion_1 = nn.MSELoss()\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Add configuration flags\n","enable_ig_loss = True\n","enable_eq_loss = False\n","\n","if (not enable_ig_loss) and (not enable_eq_loss):\n","    mode = \"Data only\"\n","    ss = 1\n","elif enable_ig_loss and (not enable_eq_loss):\n","    mode = \"Data + IG\"\n","    ss = 1 + model.parameters_size\n","elif (not enable_ig_loss) and enable_eq_loss:\n","    mode = \"Data + Eq\"\n","    ss = 1 + 1\n","else:  # enable_ig_loss and enable_eq_loss\n","    mode = \"Data + IG + Eq\"\n","    ss = 1 + model.parameters_size + 1 \n","\n","awl = AutomaticWeightedLoss(ss)\n","optimizer = optim.Adam([\n","                {'params': model.parameters(), 'lr': learning_rate},\n","                {'params': awl.parameters(), 'weight_decay': 0}\n","            ])"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Progress (Epoch 2/500, Mode: Data + IG):   0%|          | 2/500 [00:12<50:33,  6.09s/it, eq_loss=4.33e+00, eval_loss=1.74e-01, fnoloss=1.73e-01, ig_loss=6.56e+00]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m     ig_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(coieffs[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m ig_loss_list[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ig_loss_list)))\n\u001b[1;32m     80\u001b[0m     eq_loss_value \u001b[38;5;241m=\u001b[39m coieffs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m eq_loss\n\u001b[0;32m---> 82\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Accumulate losses\u001b[39;00m\n","File \u001b[0;32m/storage/work/amb10399/envs/FNO/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/storage/work/amb10399/envs/FNO/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/storage/work/amb10399/envs/FNO/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["outer_loop = tqdm(range(epochs), desc=\"Progress\", position=0)\n","torch.cuda.empty_cache()\n","\n","train_fnolosses = []\n","train_iglosses = []\n","train_eqlosses = []\n","val_losses = []\n","coieffs_list = []\n","\n","for ep in outer_loop:\n","    model.train()\n","    train_fnoloss_accumulated = 0.0\n","    train_igloss_accumulated = 0.0\n","    train_eqloss_accumulated = 0.0\n","    \n","    for batch_data in train_loader:\n","        batch_data = [item.to(device) for item in batch_data]\n","        batch_parameters, batch_u_in, batch_u_out, du_dparam_true = batch_data\n","        batch_parameters.requires_grad_(True)\n","        \n","        batch_size_ = batch_parameters.shape[0]\n","        t_tensor_ = torch.linspace(t0, t_end, steps)[T_in:].unsqueeze(0).repeat(batch_size_, 1).to(device)\n","        t_tensor_.requires_grad_(True)\n","        optimizer.zero_grad()\n","        U_in = batch_u_in\n","        U_pred = model(U_in, t_tensor_, batch_parameters).squeeze(-1)\n","        \n","        # Compute all losses first\n","        data_loss = criterion_1(U_pred, batch_u_out)\n","        \n","        # Always compute IG losses but with graphed=False when not used in loss\n","        du_dp = torch.zeros(batch_size_, T_out, model.parameters_size).to(device)\n","        du_dp = batchJacobian_AD(U_pred, batch_parameters, \n","                                graphed=(enable_ig_loss),  # Only compute gradients if IG is enabled\n","                                batchx=True)\n","        ig_loss_list = []\n","        for i in range(model.parameters_size):\n","            ig_loss_individuals = criterion_1(du_dp[:, :, i], du_dparam_true[:, T_in:, i])\n","            ig_loss_list.append(ig_loss_individuals)\n","            \n","        residual = ode_residual(U_pred, batch_parameters, t_tensor_)\n","        eq_loss = criterion_1(residual, torch.zeros_like(residual))\n","        \n","        # Case 1: Data only\n","        if (not enable_ig_loss) and (not enable_eq_loss):\n","            loss = awl(data_loss)\n","            coieffs = awl.params.data.clone().detach()\n","            fnoloss = coieffs[0].item() * data_loss\n","            ig_loss = sum(1.0 * x for x in ig_loss_list)\n","            eq_loss_value = eq_loss.item()\n","            # Clean up memory since IG is not in loss\n","            del du_dp\n","            torch.cuda.empty_cache()\n","            \n","        # Case 2: Data + IG\n","        elif enable_ig_loss and (not enable_eq_loss):\n","            loss = awl(data_loss, *[x for x in ig_loss_list])\n","            coieffs = awl.params.data.clone().detach()\n","            fnoloss = coieffs[0].item() * data_loss\n","            ig_loss = sum(coieffs[i+1].item() * ig_loss_list[i] for i in range(len(ig_loss_list)))\n","            eq_loss_value = eq_loss.item()\n","            \n","        # Case 3: Data + Eq\n","        elif (not enable_ig_loss) and enable_eq_loss:\n","            loss = awl(data_loss, eq_loss)\n","            coieffs = awl.params.data.clone().detach()\n","            fnoloss = coieffs[0].item() * data_loss\n","            ig_loss = sum(1.0 * x for x in ig_loss_list)\n","            eq_loss_value = coieffs[-1].item() * eq_loss\n","            # Clean up memory since IG is not in loss\n","            del du_dp\n","            torch.cuda.empty_cache()\n","            \n","        # Case 4: Data + IG + Eq\n","        else:  # enable_ig_loss and enable_eq_loss\n","            loss = awl(data_loss, *[x for x in ig_loss_list], eq_loss)\n","            coieffs = awl.params.data.clone().detach()\n","            fnoloss = coieffs[0].item() * data_loss\n","            ig_loss = sum(coieffs[i+1].item() * ig_loss_list[i] for i in range(len(ig_loss_list)))\n","            eq_loss_value = coieffs[-1].item() * eq_loss\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Accumulate losses\n","        train_fnoloss_accumulated += fnoloss.item() * batch_size_\n","        train_igloss_accumulated += ig_loss * batch_size_\n","        train_eqloss_accumulated += eq_loss_value * batch_size_\n","    \n","    coieffs_list.append(coieffs)\n","    epoch_fnoloss = train_fnoloss_accumulated / len(train_loader.dataset)\n","    epoch_igloss = train_igloss_accumulated / len(train_loader.dataset)\n","    epoch_eqloss = train_eqloss_accumulated / len(train_loader.dataset)\n","    \n","    train_fnolosses.append(epoch_fnoloss)\n","    train_iglosses.append(epoch_igloss)\n","    train_eqlosses.append(epoch_eqloss)\n","    \n","    # Evaluation phase\n","    model.eval()\n","    val_loss_accumulated = 0.0\n","    with torch.no_grad():\n","        for batch_data in eval_loader:\n","            batch_data = [item.to(device) for item in batch_data]\n","            batch_parameters, batch_u_in, batch_u_out = batch_data[:3]\n","            batch_size_ = batch_parameters.shape[0]\n","            t_tensor_ = torch.linspace(t0, t_end, steps)[T_in:].unsqueeze(0).repeat(batch_size_, 1).to(device)\n","            U_in = batch_u_in\n","            U_pred = model(U_in, t_tensor_, batch_parameters).squeeze(-1)\n","            val_fnoloss = criterion_1(U_pred, batch_u_out)\n","            val_loss_accumulated += val_fnoloss.item() * batch_size_\n","        epoch_val_loss = val_loss_accumulated / len(eval_loader.dataset)\n","        val_losses.append(epoch_val_loss)\n","\n","    losses_dict = {\n","        'Training FNO Loss': train_fnolosses,\n","        'Training IG Loss': train_iglosses,\n","        'Training EQ Loss': train_eqlosses,\n","        'Validation Loss': val_losses\n","    }\n","    \n","    if ep % 10 == 0:\n","        torch.save({\n","            'epoch': ep,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","        }, PATH + 'saved_model/ODE_2_saved_Model_Data_IG_EQ_saved.pth')\n","    \n","    outer_loop.set_description(f\"Progress (Epoch {ep + 1}/{epochs}, Mode: {mode})\")\n","    outer_loop.set_postfix(\n","        fnoloss=f'{epoch_fnoloss:.2e}',\n","        ig_loss=f'{epoch_igloss:.2e}',\n","        eq_loss=f'{epoch_eqloss:.2e}',\n","        eval_loss=f'{epoch_val_loss:.2e}'\n","    )\n","\n","print(\"Training complete\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":2}
