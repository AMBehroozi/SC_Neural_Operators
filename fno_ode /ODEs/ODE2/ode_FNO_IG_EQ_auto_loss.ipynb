{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, dataset\n",
    "import sourcedefender\n",
    "import random\n",
    "   \n",
    "sys.path.append(\"../../../\")\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "from lib.util import MHPI, count_parameters\n",
    "from lib.utiltools import loss_live_plot, GaussianRandomFieldGenerator, generate_batch_parameters, AutomaticWeightedLoss\n",
    "from lib.DerivativeComputer import batchJacobian_AD\n",
    "from models.FNO_1d_simple import FNO1d\n",
    "from equation.ode2 import *\n",
    "\n",
    "MHPI()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}\\n')\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_number = 1000\n",
    "training_sample = 10\n",
    "dataset_segment_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_live_loss = True\n",
    "Create_new_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = 8\n",
    "width = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ = 1.5\n",
    "tau = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = 0.0\n",
    "t_end = 2\n",
    "steps = 100\n",
    "T_in = 10\n",
    "T_out = steps - T_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer and training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "scheduler_step = 100\n",
    "scheduler_gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time tensor\n",
    "t_tensor = torch.linspace(t0, t_end, steps)  # Shape: [Steps]\n",
    "\n",
    "# Define parameter ranges\n",
    "alpha_range = (0.02, 0.06)\n",
    "beta_range = (0.01, 0.03)\n",
    "gamma_range = (20, 60)\n",
    "delta_range = (0.5, 1.5)\n",
    "omega_range = (0.2, 0.6)\n",
    "epsilon_range = (0.0, 0.2)\n",
    "zeta_range = (0.0, 0.2)\n",
    "\n",
    "# Generate random samples for each parameter\n",
    "alpha = alpha_range[0] + (alpha_range[1] - alpha_range[0]) * torch.rand(Sample_number)\n",
    "beta = beta_range[0] + (beta_range[1] - beta_range[0]) * torch.rand(Sample_number)\n",
    "gamma = gamma_range[0] + (gamma_range[1] - gamma_range[0]) * torch.rand(Sample_number)\n",
    "delta = delta_range[0] + (delta_range[1] - delta_range[0]) * torch.rand(Sample_number)\n",
    "omega = omega_range[0] + (omega_range[1] - omega_range[0]) * torch.rand(Sample_number)\n",
    "epsilon = epsilon_range[0] + (epsilon_range[1] - epsilon_range[0]) * torch.rand(Sample_number)\n",
    "zeta = zeta_range[0] + (zeta_range[1] - zeta_range[0]) * torch.rand(Sample_number)\n",
    "\n",
    "# Stack all parameters into a single tensor\n",
    "parameters = torch.stack([alpha, beta, gamma, delta, omega, epsilon, zeta], dim=1).requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ''\n",
    "# Create dataset < This part can be changed based on different cases>\n",
    "if Create_new_dataset:\n",
    "    dataset = creat_dataset(t_tensor, parameters, T_in)\n",
    "    torch.save(dataset, PATH + 'datasets/main_dataset_ODE2.pt')\n",
    "    # Calculate sizes for train/eval/test split\n",
    "    train_size = int(0.75 * Sample_number)\n",
    "    eval_size = int(0.15 * Sample_number)\n",
    "    test_size = Sample_number - train_size - eval_size\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset, eval_dataset, test_dataset = random_split(dataset, [train_size, eval_size, test_size]) # Split the dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset)\n",
    "    torch.save(train_loader, PATH + 'datasets/train_dataset_ODE2.pt')\n",
    "    torch.save(eval_loader, PATH + 'datasets/eval_dataset_ODE2.pt')\n",
    "    torch.save(test_loader, PATH + 'datasets//test_dataset_ODE2.pt')\n",
    "\n",
    "else:\n",
    "    dataset = torch.load(PATH + 'datasets/main_dataset_ODE2.pt')\n",
    "    train_loader = torch.load(PATH + 'datasets/train_dataset_ODE2.pt')\n",
    "    eval_loader = torch.load(PATH + 'datasets/eval_dataset_ODE2.pt')\n",
    "    test_loader = torch.load(PATH + 'datasets/test_dataset_ODE2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNO1d(modes, width, T_in, T_out, state_size=1, parameters_size=7).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=250, gamma=1.025)\n",
    "train_fnolosses, train_odelosses, train_iglosses, val_losses = [], [], [], []\n",
    "coieffs_list = []\n",
    "criterion_1 = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add configuration flags\n",
    "enable_ig_loss = True\n",
    "enable_eq_loss = False\n",
    "\n",
    "if (not enable_ig_loss) and (not enable_eq_loss):\n",
    "    mode = \"Data only\"\n",
    "    ss = 1\n",
    "elif enable_ig_loss and (not enable_eq_loss):\n",
    "    mode = \"Data + IG\"\n",
    "    ss = 1 + model.parameters_size\n",
    "elif (not enable_ig_loss) and enable_eq_loss:\n",
    "    mode = \"Data + Eq\"\n",
    "    ss = 1 + 1\n",
    "else:  # enable_ig_loss and enable_eq_loss\n",
    "    mode = \"Data + IG + Eq\"\n",
    "    ss = 1 + model.parameters_size + 1 \n",
    "\n",
    "awl = AutomaticWeightedLoss(ss)\n",
    "optimizer = optim.Adam([\n",
    "                {'params': model.parameters(), 'lr': learning_rate},\n",
    "                {'params': awl.parameters(), 'weight_decay': 0}\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_loop = tqdm(range(epochs), desc=\"Progress\", position=0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_fnolosses = []\n",
    "train_iglosses = []\n",
    "train_eqlosses = []\n",
    "val_losses = []\n",
    "coieffs_list = []\n",
    "\n",
    "for ep in outer_loop:\n",
    "    model.train()\n",
    "    train_fnoloss_accumulated = 0.0\n",
    "    train_igloss_accumulated = 0.0\n",
    "    train_eqloss_accumulated = 0.0\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        batch_data = [item.to(device) for item in batch_data]\n",
    "        batch_parameters, batch_u_in, batch_u_out, du_dparam_true = batch_data\n",
    "        batch_parameters.requires_grad_(True)\n",
    "        \n",
    "        batch_size_ = batch_parameters.shape[0]\n",
    "        t_tensor_ = torch.linspace(t0, t_end, steps)[T_in:].unsqueeze(0).repeat(batch_size_, 1).to(device)\n",
    "        t_tensor_.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "        U_in = batch_u_in\n",
    "        U_pred = model(U_in, t_tensor_, batch_parameters).squeeze(-1)\n",
    "        \n",
    "        # Compute all losses first\n",
    "        data_loss = criterion_1(U_pred, batch_u_out.squeeze(-1))\n",
    "        \n",
    "        # Always compute IG losses but with graphed=False when not used in loss\n",
    "        du_dp = torch.zeros(batch_size_, T_out, model.parameters_size).to(device)\n",
    "        du_dp = batchJacobian_AD(U_pred, batch_parameters, \n",
    "                                graphed=(enable_ig_loss),  # Only compute gradients if IG is enabled\n",
    "                                batchx=True)\n",
    "        ig_loss_list = []\n",
    "        for i in range(model.parameters_size):\n",
    "            ig_loss_individuals = criterion_1(du_dp[:, :, i], du_dparam_true.squeeze(-1)[:, T_in:, i])\n",
    "            ig_loss_list.append(ig_loss_individuals)\n",
    "            \n",
    "        residual = ode_residual(U_pred, batch_parameters, t_tensor_)\n",
    "        eq_loss = criterion_1(residual, torch.zeros_like(residual))\n",
    "        \n",
    "        # Case 1: Data only\n",
    "        if (not enable_ig_loss) and (not enable_eq_loss):\n",
    "            loss = awl(data_loss)\n",
    "            coieffs = awl.params.data.clone().detach()\n",
    "            fnoloss = coieffs[0].item() * data_loss\n",
    "            ig_loss = sum(1.0 * x for x in ig_loss_list)\n",
    "            eq_loss_value = eq_loss.item()\n",
    "            # Clean up memory since IG is not in loss\n",
    "            del du_dp\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Case 2: Data + IG\n",
    "        elif enable_ig_loss and (not enable_eq_loss):\n",
    "            loss = awl(data_loss, *[x for x in ig_loss_list])\n",
    "            coieffs = awl.params.data.clone().detach()\n",
    "            fnoloss = coieffs[0].item() * data_loss\n",
    "            ig_loss = sum(coieffs[i+1].item() * ig_loss_list[i] for i in range(len(ig_loss_list)))\n",
    "            eq_loss_value = eq_loss.item()\n",
    "            \n",
    "        # Case 3: Data + Eq\n",
    "        elif (not enable_ig_loss) and enable_eq_loss:\n",
    "            loss = awl(data_loss, eq_loss)\n",
    "            coieffs = awl.params.data.clone().detach()\n",
    "            fnoloss = coieffs[0].item() * data_loss\n",
    "            ig_loss = sum(1.0 * x for x in ig_loss_list)\n",
    "            eq_loss_value = coieffs[-1].item() * eq_loss\n",
    "            # Clean up memory since IG is not in loss\n",
    "            del du_dp\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Case 4: Data + IG + Eq\n",
    "        else:  # enable_ig_loss and enable_eq_loss\n",
    "            loss = awl(data_loss, *[x for x in ig_loss_list], eq_loss)\n",
    "            coieffs = awl.params.data.clone().detach()\n",
    "            fnoloss = coieffs[0].item() * data_loss\n",
    "            ig_loss = sum(coieffs[i+1].item() * ig_loss_list[i] for i in range(len(ig_loss_list)))\n",
    "            eq_loss_value = coieffs[-1].item() * eq_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        train_fnoloss_accumulated += fnoloss.item() * batch_size_\n",
    "        train_igloss_accumulated += ig_loss * batch_size_\n",
    "        train_eqloss_accumulated += eq_loss_value * batch_size_\n",
    "    \n",
    "    coieffs_list.append(coieffs)\n",
    "    epoch_fnoloss = train_fnoloss_accumulated / len(train_loader.dataset)\n",
    "    epoch_igloss = train_igloss_accumulated / len(train_loader.dataset)\n",
    "    epoch_eqloss = train_eqloss_accumulated / len(train_loader.dataset)\n",
    "    \n",
    "    train_fnolosses.append(epoch_fnoloss)\n",
    "    train_iglosses.append(epoch_igloss)\n",
    "    train_eqlosses.append(epoch_eqloss)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    val_loss_accumulated = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in eval_loader:\n",
    "            batch_data = [item.to(device) for item in batch_data]\n",
    "            batch_parameters, batch_u_in, batch_u_out = batch_data[:3]\n",
    "            batch_size_ = batch_parameters.shape[0]\n",
    "            t_tensor_ = torch.linspace(t0, t_end, steps)[T_in:].unsqueeze(0).repeat(batch_size_, 1).to(device)\n",
    "            U_in = batch_u_in\n",
    "            U_pred = model(U_in, t_tensor_, batch_parameters).squeeze(-1)\n",
    "            val_fnoloss = criterion_1(U_pred, batch_u_out.squeeze(-1))\n",
    "            val_loss_accumulated += val_fnoloss.item() * batch_size_\n",
    "        epoch_val_loss = val_loss_accumulated / len(eval_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "    losses_dict = {\n",
    "        'Training FNO Loss': train_fnolosses,\n",
    "        'Training IG Loss': train_iglosses,\n",
    "        'Training EQ Loss': train_eqlosses,\n",
    "        'Validation Loss': val_losses\n",
    "    }\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': ep,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, PATH + 'saved_model/ODE_2_saved_Model_Data_IG_EQ_saved.pth')\n",
    "    \n",
    "    outer_loop.set_description(f\"Progress (Epoch {ep + 1}/{epochs}, Mode: {mode})\")\n",
    "    outer_loop.set_postfix(\n",
    "        fnoloss=f'{epoch_fnoloss:.2e}',\n",
    "        ig_loss=f'{epoch_igloss:.2e}',\n",
    "        eq_loss=f'{epoch_eqloss:.2e}',\n",
    "        eval_loss=f'{epoch_val_loss:.2e}'\n",
    "    )\n",
    "\n",
    "print(\"Training complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
